# ─────────────────────────────────────────────────────────────────────────────
# Cloud Run v2 Service — Lumen ML Pipeline (GPU-enabled)
# ─────────────────────────────────────────────────────────────────────────────
# Declarative service definition deployed via:
#   gcloud run services replace infrastructure/service.yaml \
#     --project=lumen-pipeline --region=us-central1
#
# This is the single source of truth for the Cloud Run service config.
# Terraform manages supporting infra (buckets, IAM, secrets, Artifact Registry).
# ─────────────────────────────────────────────────────────────────────────────

apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: lumen-pipeline
  labels:
    cloud.googleapis.com/location: us-central1
  annotations:
    run.googleapis.com/launch-stage: BETA
    run.googleapis.com/ingress: all
spec:
  template:
    metadata:
      annotations:
        run.googleapis.com/cpu-throttling: "false"
        run.googleapis.com/startup-cpu-boost: "true"
        run.googleapis.com/gpu-zonal-redundancy-disabled: "true"
        autoscaling.knative.dev/minScale: "1"
        autoscaling.knative.dev/maxScale: "1"
        run.googleapis.com/container-dependencies: "{}"
    spec:
      containerConcurrency: 1
      timeoutSeconds: 600
      serviceAccountName: lumen-pipeline-sa@lumen-pipeline.iam.gserviceaccount.com
      nodeSelector:
        run.googleapis.com/accelerator: nvidia-rtx-pro-6000
      containers:
        - image: us-central1-docker.pkg.dev/lumen-pipeline/lumen-pipeline-docker/lumen-pipeline:latest
          ports:
            - containerPort: 8080

          env:
            - name: CACHE_BUCKET
              value: lumen-shape-cache-lumen-pipeline
            - name: MODEL_CACHE_DIR
              value: /home/appuser/models
            - name: MODEL_WEIGHTS_BUCKET
              value: lumen-model-weights-lumen-pipeline
            - name: ALLOWED_ORIGINS
              value: "https://storage.googleapis.com,http://localhost:5173,https://embers-camp.web.app,https://embers.camp"
            - name: EAGER_LOAD_ALL
              value: "true"
            - name: API_KEY
              valueFrom:
                secretKeyRef:
                  name: lumen-api-key
                  key: latest
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: lumen-hf-token
                  key: latest

          resources:
            limits:
              cpu: "20"
              memory: 80Gi
              nvidia.com/gpu: "1"

          # Startup probe: checks models loaded + cache connected.
          # Returns 503 until ready, so Cloud Run won't route traffic too early.
          # Budget: 120s delay + (30 × 60s) = 1920s (~32 min) to accommodate
          # GCS weight sync + model loading to GPU (PyTorch 2.8 + Blackwell init).
          startupProbe:
            httpGet:
              path: /health/ready
              port: 8080
            initialDelaySeconds: 120
            periodSeconds: 60
            failureThreshold: 30
            timeoutSeconds: 30

          # Liveness probe: near-zero cost, just confirms the process is alive.
          # Failure triggers container restart.
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 5

  traffic:
    - percent: 100
      latestRevision: true
